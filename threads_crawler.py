import json
import re
from datetime import datetime
from enum import Enum
from typing import Dict, Optional, Tuple
from urllib.parse import urlparse, parse_qs, unquote
import time

import jmespath
from fake_useragent import UserAgent
from nested_lookup import nested_lookup
from parsel import Selector
from selenium import webdriver
from selenium.webdriver import ChromeOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from selenium.common.exceptions import TimeoutException, WebDriverException

from sns_info import SnsInfo, Profile


class MediaType(Enum):
    IMAGE = 1
    VIDEO = 2
    CAROUSEL_ALBUM = 8
    TEXT_POST = 19


def get_chrome_options() -> ChromeOptions:
    """å»ºç«‹ç©©å®šçš„ Chrome é¸é …è¨­å®š"""
    chrome_options = ChromeOptions()

    # åŸºæœ¬è¨­å®š
    chrome_options.add_argument("--headless=new")  # ä½¿ç”¨æ–°ç‰ˆ headless æ¨¡å¼
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")  # è§£æ±º Docker ä¸­çš„ shared memory å•é¡Œ

    # æ•ˆèƒ½å„ªåŒ–
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("--disable-software-rasterizer")

    # ç©©å®šæ€§è¨­å®š
    chrome_options.add_argument("--disable-crash-reporter")
    chrome_options.add_argument("--disable-in-process-stack-traces")
    chrome_options.add_argument("--disable-logging")
    chrome_options.add_argument("--log-level=3")
    chrome_options.add_argument("--silent")

    # è¨˜æ†¶é«”ç®¡ç†
    chrome_options.add_argument("--disable-background-networking")
    chrome_options.add_argument("--disable-background-timer-throttling")
    chrome_options.add_argument("--disable-backgrounding-occluded-windows")
    chrome_options.add_argument("--disable-breakpad")
    chrome_options.add_argument("--disable-component-extensions-with-background-pages")
    chrome_options.add_argument("--disable-features=TranslateUI,BlinkGenPropertyTrees")
    chrome_options.add_argument("--disable-ipc-flooding-protection")
    chrome_options.add_argument("--disable-renderer-backgrounding")

    # è¦–çª—è¨­å®š
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--hide-scrollbars")
    chrome_options.add_argument("--mute-audio")

    # User Agent
    ua = UserAgent()
    chrome_options.add_argument(f"user-agent={ua.random}")

    # é é¢è¼‰å…¥ç­–ç•¥
    chrome_options.page_load_strategy = 'eager'  # ä¸ç­‰å¾…æ‰€æœ‰è³‡æºè¼‰å…¥å®Œæˆ

    # å¯¦é©—æ€§é¸é …
    chrome_options.add_experimental_option("excludeSwitches", ["enable-logging", "enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)

    # åå¥½è¨­å®š
    prefs = {
        "profile.managed_default_content_settings.images": 2,  # ä¸è¼‰å…¥åœ–ç‰‡ä»¥ç¯€çœè³‡æº
        "profile.default_content_setting_values.notifications": 2,
        "profile.managed_default_content_settings.stylesheets": 2,
    }
    chrome_options.add_experimental_option("prefs", prefs)

    return chrome_options


def parse_thread(data: Dict) -> Dict:
    """Parse Threads post JSON dataset for the most important fields"""
    result = jmespath.search(
        """{
        text: caption.text,
        published_on: taken_at,
        id: id,
        pk: pk,
        code: code,
        username: user.username,
        user_pic: user.profile_pic_url,
        user_pk: user.pk,
        user_id: user.id,
        carousel_media: carousel_media,
        attachment: text_post_app_info.link_preview_attachment.url,
        post_image: image_versions2.candidates[0].url,
        post_video: video_versions[0].url,
        media_type: media_type,
        linked_inline_media: text_post_app_info.linked_inline_media,
        share_info: text_post_app_info.share_info
    }""",
        data
    )

    all_images, all_videos = extract_all_media(result)
    result["all_images"] = all_images
    result["all_videos"] = all_videos
    result["url"] = f"https://www.threads.com/@{result['username']}/post/{result['code']}"

    print(result)
    return result


def extract_all_media(result):
    all_images = []
    all_videos = []

    media_type = result.get("media_type")

    if media_type == MediaType.IMAGE.value:
        if result.get("post_image") and not result.get("post_video"):
            all_images.append(result["post_image"])

    elif media_type == MediaType.VIDEO.value:
        if result.get("post_video"):
            all_videos.append(result["post_video"])

    elif media_type == MediaType.CAROUSEL_ALBUM.value:
        for media in result.get("carousel_media", []):
            if media.get("video_versions"):
                all_videos.append(media["video_versions"][0]["url"])
            else:
                all_images.append(media["image_versions2"]["candidates"][0]["url"])

    elif media_type == MediaType.TEXT_POST.value:
        linked_inline_media = result.get("linked_inline_media")
        if linked_inline_media:
            linked_media_type = linked_inline_media.get("media_type")
            if linked_media_type == MediaType.VIDEO.value:
                all_videos.append(linked_inline_media["video_versions"][0]["url"])
            elif linked_media_type == MediaType.IMAGE.value:
                all_images.append(linked_inline_media["image_versions2"]["candidates"][0]["url"])
            elif linked_media_type == MediaType.CAROUSEL_ALBUM.value:
                for media in linked_inline_media.get("carousel_media", []):
                    if media.get("video_versions"):
                        all_videos.append(media["video_versions"][0]["url"])
                    else:
                        all_images.append(media["image_versions2"]["candidates"][0]["url"])
        elif result.get("attachment"):
            all_videos.append(extract_original_url(result["attachment"]))

    return all_images, all_videos


def extract_original_url(threads_url: str) -> str:
    """å¾ Threads çš„è·³è½‰ URL ä¸­æå–åŸå§‹ URL"""
    parsed_url = urlparse(threads_url)
    query_params = parse_qs(parsed_url.query)
    return unquote(query_params.get("u", [""])[0])


def scrape_thread(url: str, max_retries: int = 3) -> dict:
    """
    çˆ¬å– Threads è²¼æ–‡è³‡æ–™ï¼Œå¢åŠ éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶

    Args:
        url: Threads è²¼æ–‡ URL
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸

    Returns:
        è²¼æ–‡è³‡æ–™å­—å…¸ï¼Œå¤±æ•—æ™‚è¿”å›ç©ºå­—å…¸
    """
    pattern = r"threads\.com/@([\w.]+)/post/([\w-]+)"
    match = re.search(pattern, url)
    if not match:
        print(f"âŒ URL æ ¼å¼éŒ¯èª¤: {url}")
        return {}

    username, post_code = match.groups()
    driver = None

    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ å˜—è©¦ {attempt + 1}/{max_retries}...")

            # å»ºç«‹ WebDriver
            chrome_options = get_chrome_options()
            driver = webdriver.Chrome(options=chrome_options)

            # è¨­å®šè¶…æ™‚
            driver.set_page_load_timeout(30)
            driver.set_script_timeout(30)

            # è¨ªå•é é¢
            driver.get(url)

            # ç­‰å¾…é—œéµå…ƒç´ è¼‰å…¥
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, 'script[type="application/json"][data-sjs]')
                )
            )

            # çµ¦é é¢ä¸€é»æ™‚é–“å®Œæˆ JavaScript åŸ·è¡Œ
            time.sleep(2)

            # ææ—©æŠ“å– page source
            page_source = driver.page_source

            # ç«‹å³é—œé–‰ç€è¦½å™¨é‡‹æ”¾è³‡æº
            driver.quit()
            driver = None

            # è§£æè³‡æ–™
            selector = Selector(page_source)
            hidden_datasets = selector.css('script[type="application/json"][data-sjs]::text').getall()

            thread_items = []
            for hidden_dataset in hidden_datasets:
                if '"ScheduledServerJS"' not in hidden_dataset or "thread_items" not in hidden_dataset:
                    continue

                try:
                    data = json.loads(hidden_dataset)
                    temp_thread_items = nested_lookup("thread_items", data)
                    thread_items.extend(temp_thread_items)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  JSON è§£æéŒ¯èª¤: {e}")
                    continue

            if thread_items:
                result = next(
                    (parse_thread(thread["post"])
                     for item in thread_items
                     for thread in item
                     if thread["post"]["user"]["username"] == username
                     and thread["post"]["code"] == post_code),
                    None
                )

                if result:
                    print("âœ… æˆåŠŸçˆ¬å–è³‡æ–™")
                    return result
                else:
                    print("âš ï¸  æ‰¾ä¸åˆ°åŒ¹é…çš„è²¼æ–‡")

            else:
                print("âš ï¸  é é¢ä¸­æ²’æœ‰æ‰¾åˆ° thread_items")

            # å¦‚æœåŸ·è¡Œåˆ°é€™è£¡è¡¨ç¤ºæ²’æœ‰è¿”å›è³‡æ–™ï¼Œä½†ä¹Ÿæ²’æœ‰éŒ¯èª¤ï¼Œç­‰å¾…å¾Œé‡è©¦
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                print(f"â³ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)

        except TimeoutException as e:
            print(f"âŒ è¶…æ™‚éŒ¯èª¤: {e}")
            if driver:
                try:
                    driver.quit()
                except:
                    pass
                driver = None

            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 3
                print(f"â³ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)

        except WebDriverException as e:
            print(f"âŒ WebDriver éŒ¯èª¤: {e}")
            if driver:
                try:
                    driver.quit()
                except:
                    pass
                driver = None

            # é€™ç¨®éŒ¯èª¤é€šå¸¸æ˜¯ç€è¦½å™¨å´©æ½°ï¼Œéœ€è¦è¼ƒé•·çš„ç­‰å¾…æ™‚é–“
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5
                print(f"â³ WebDriver éŒ¯èª¤ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)

        except Exception as e:
            print(f"âŒ æœªé æœŸçš„éŒ¯èª¤: {e}")
            if driver:
                try:
                    driver.quit()
                except:
                    pass
                driver = None

            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                print(f"â³ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)

        finally:
            # ç¢ºä¿ driver è¢«é—œé–‰
            if driver:
                try:
                    driver.quit()
                except:
                    pass

    print(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†")
    return {}


def fetch_data_from_browser(url: str) -> Tuple[Optional[SnsInfo], Optional[SnsInfo]]:
    """
    å¾ç€è¦½å™¨çˆ¬å–è³‡æ–™ä¸¦è½‰æ›ç‚º SnsInfo

    Returns:
        (ä¸»è²¼æ–‡, å¼•ç”¨è²¼æ–‡) çš„ tupleï¼Œå¤±æ•—æ™‚è¿”å› (None, None)
    """
    main_post = scrape_thread(url)
    if not main_post:
        print("âŒ ç„¡æ³•çˆ¬å–ä¸»è²¼æ–‡")
        return None, None

    quoted_post = None
    if main_post.get("share_info"):
        quoted = main_post["share_info"].get("quoted_post")
        if quoted and quoted.get("code"):
            try:
                quoted_post = convert_to_sns_info(parse_thread(quoted))
            except Exception as e:
                print(f"âš ï¸  è™•ç†å¼•ç”¨è²¼æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return convert_to_sns_info(main_post), quoted_post


def convert_to_sns_info(thread: Dict) -> SnsInfo:
    """å°‡ thread è³‡æ–™è½‰æ›ç‚º SnsInfo ç‰©ä»¶"""
    return SnsInfo(
        post_link=thread["url"],
        profile=Profile(name=thread["username"], url=thread["user_pic"]),
        content=thread["text"],
        images=(thread.get("all_images") or []),
        videos=(thread.get("all_videos") or []),
        timestamp=datetime.fromtimestamp(thread["published_on"])
    )


if __name__ == "__main__":
    # æ¸¬è©¦
    test_url = "https://www.threads.com/@cryforyysh/post/DQBQiuXjv-u"

    print("=" * 60)
    print("ğŸš€ é–‹å§‹çˆ¬å– Threads è²¼æ–‡")
    print("=" * 60)

    sns_info, share_info = fetch_data_from_browser(test_url)

    if sns_info:
        print("\nâœ… ä¸»è²¼æ–‡:")
        print(sns_info)
    else:
        print("\nâŒ ç„¡æ³•å–å¾—ä¸»è²¼æ–‡")

    if share_info:
        print("\nâœ… å¼•ç”¨è²¼æ–‡:")
        print(share_info)