import asyncio
import json
import re
from datetime import datetime
from enum import Enum
from typing import Dict, Optional, Tuple
from urllib.parse import urlparse, parse_qs, unquote
import base64

import jmespath
from nested_lookup import nested_lookup
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
import aiohttp

from sns_info import SnsInfo, Profile


class MediaType(Enum):
    IMAGE = 1
    VIDEO = 2
    CAROUSEL_ALBUM = 8
    TEXT_POST = 19


def parse_thread(data: Dict) -> Dict:
    """Parse Threads post JSON dataset for the most important fields"""
    result = jmespath.search(
        """{
        text: caption.text,
        published_on: taken_at,
        id: id,
        pk: pk,
        code: code,
        username: user.username,
        user_pic: user.profile_pic_url,
        user_pk: user.pk,
        user_id: user.id,
        carousel_media: carousel_media,
        attachment: text_post_app_info.link_preview_attachment.url,
        post_image: image_versions2.candidates[0].url,
        post_video: video_versions[0].url,
        media_type: media_type,
        linked_inline_media: text_post_app_info.linked_inline_media,
        share_info: text_post_app_info.share_info
    }""",
        data
    )

    all_images, all_videos, all_attachments = extract_all_media(result)
    result["all_images"] = all_images
    result["all_videos"] = all_videos
    result["all_attachments"] = all_attachments
    result["url"] = f"https://www.threads.com/@{result['username']}/post/{result['code']}"

    print(result)
    return result


def extract_all_media(result):
    all_images = []
    all_videos = []
    all_attachments = []

    media_type = result.get("media_type")

    if media_type == MediaType.IMAGE.value:
        if result.get("post_image") and not result.get("post_video"):
            all_images.append(result["post_image"])

    elif media_type == MediaType.VIDEO.value:
        if result.get("post_video"):
            all_videos.append(result["post_video"])

    elif media_type == MediaType.CAROUSEL_ALBUM.value:
        for media in result.get("carousel_media", []):
            if media.get("video_versions"):
                all_videos.append(media["video_versions"][0]["url"])
            else:
                all_images.append(media["image_versions2"]["candidates"][0]["url"])

    elif media_type == MediaType.TEXT_POST.value:
        linked_inline_media = result.get("linked_inline_media")
        if linked_inline_media:
            linked_media_type = linked_inline_media.get("media_type")
            if linked_media_type == MediaType.VIDEO.value:
                all_videos.append(linked_inline_media["video_versions"][0]["url"])
            elif linked_media_type == MediaType.IMAGE.value:
                all_images.append(linked_inline_media["image_versions2"]["candidates"][0]["url"])
            elif linked_media_type == MediaType.CAROUSEL_ALBUM.value:
                for media in linked_inline_media.get("carousel_media", []):
                    if media.get("video_versions"):
                        all_videos.append(media["video_versions"][0]["url"])
                    else:
                        all_images.append(media["image_versions2"]["candidates"][0]["url"])
        elif result.get("attachment"):
            all_attachments.append(extract_original_url(result["attachment"]))

    return all_images, all_videos, all_attachments


def extract_original_url(threads_url: str) -> str:
    """å¾ Threads çš„è·³è½‰ URL ä¸­æå–åŸå§‹ URL"""
    parsed_url = urlparse(threads_url)
    query_params = parse_qs(parsed_url.query)
    return unquote(query_params.get("u", [""])[0])


async def upload_to_imgur(image_path: str) -> Optional[str]:
    """
    ä¸Šå‚³åœ–ç‰‡åˆ° Imgur

    Args:
        image_path: åœ–ç‰‡æª”æ¡ˆè·¯å¾‘

    Returns:
        åœ–ç‰‡ URLï¼Œå¤±æ•—æ™‚è¿”å› None
    """
    try:
        # Imgur åŒ¿åä¸Šå‚³ API (ç„¡éœ€è¨»å†Š)
        # é€™æ˜¯ Imgur çš„å…¬é–‹ Client IDï¼Œåƒ…ä¾›åŒ¿åä¸Šå‚³ä½¿ç”¨
        client_id = "546c25a59c58ad7"

        with open(image_path, 'rb') as f:
            image_data = base64.b64encode(f.read()).decode('utf-8')

        headers = {
            'Authorization': f'Client-ID {client_id}'
        }

        data = {
            'image': image_data,
            'type': 'base64'
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                    'https://api.imgur.com/3/image',
                    headers=headers,
                    data=data,
                    timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    if result.get('success'):
                        image_url = result['data']['link']
                        print(f"â˜ï¸  åœ–ç‰‡å·²ä¸Šå‚³åˆ° Imgur: {image_url}")
                        return image_url
                    else:
                        print(f"âŒ Imgur ä¸Šå‚³å¤±æ•—: {result}")
                else:
                    print(f"âŒ Imgur ä¸Šå‚³å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status}")

    except Exception as e:
        print(f"âŒ ä¸Šå‚³åœ–ç‰‡åˆ° Imgur æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return None


async def scrape_thread(url: str, max_retries: int = 1) -> dict:
    """
    çˆ¬å– Threads è²¼æ–‡è³‡æ–™ï¼Œå¢åŠ éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶

    Args:
        url: Threads è²¼æ–‡ URL
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸

    Returns:
        è²¼æ–‡è³‡æ–™å­—å…¸ï¼Œå¤±æ•—æ™‚è¿”å›ç©ºå­—å…¸
    """
    pattern = r"threads\.com/@([\w.]+)/post/([\w-]+)"
    match = re.search(pattern, url)
    if not match:
        print(f"âŒ URL æ ¼å¼éŒ¯èª¤: {url}")
        return {}

    username, post_code = match.groups()

    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ å˜—è©¦ {attempt + 1}/{max_retries}...")

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()

                # ä½¿ç”¨å¿«é€Ÿè¼‰å…¥ï¼Œåªç­‰å¾… DOM è¼‰å…¥å®Œæˆ
                await page.goto(url, wait_until="domcontentloaded")

                scripts = await page.locator("script[type='application/json']").all()

                thread_items = []

                for i, script in enumerate(scripts):
                    content = await script.inner_text()

                    if '"ScheduledServerJS"' not in content or "thread_items" not in content:
                        continue

                    try:
                        data = json.loads(content)
                        thread_items.extend(nested_lookup("thread_items", data))
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸  JSON è§£æéŒ¯èª¤: {e}")
                        continue

                if thread_items:
                    result = next(
                        (parse_thread(thread["post"])
                         for item in thread_items
                         for thread in item
                         if thread["post"]["user"]["username"] == username
                         and thread["post"]["code"] == post_code),
                        None
                    )

                    if result:
                        print("âœ… æˆåŠŸçˆ¬å–è³‡æ–™")
                        await browser.close()
                        return result
                    else:
                        print("âš ï¸  æ‰¾ä¸åˆ°åŒ¹é…çš„è²¼æ–‡")
                        await browser.close()
                        # ä¸åšæˆªåœ–ï¼Œç›´æ¥è¿”å›ç©ºå­—å…¸

                else:
                    print("âš ï¸  é é¢ä¸­æ²’æœ‰æ‰¾åˆ° thread_itemsï¼Œæ“·å–å¿«ç…§ä¸¦ä¸Šå‚³...")

                    # åªæœ‰åœ¨é€™å€‹æƒ…æ³ä¸‹æ‰åšå®Œæ•´çš„é é¢è¼‰å…¥å’Œæˆªåœ–
                    try:
                        # ç­‰å¾…é é¢å®Œå…¨è¼‰å…¥
                        await page.wait_for_load_state("networkidle", timeout=10000)
                        await asyncio.sleep(2)

                        # å–å¾—é é¢æ¨™é¡Œå’Œ URL ç¢ºèªé é¢æœ‰è¼‰å…¥
                        page_title = await page.title()
                        current_url = page.url
                        print(f"ğŸ“„ é é¢æ¨™é¡Œ: {page_title}")
                        print(f"ğŸ”— ç•¶å‰ URL: {current_url}")

                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        screenshot_path = f"threads_screenshot_{timestamp}.png"
                        html_path = f"threads_page_{timestamp}.html"

                        # å„²å­˜ HTML å…§å®¹
                        html_content = await page.content()
                        with open(html_path, 'w', encoding='utf-8') as f:
                            f.write(html_content)
                        print(f"ğŸ’¾ HTML å·²å„²å­˜: {html_path}")

                        # æˆªå–å®Œæ•´é é¢
                        await page.screenshot(path=screenshot_path, full_page=True)
                        print(f"ğŸ“¸ å¿«ç…§å·²å„²å­˜: {screenshot_path}")

                        # ä¸Šå‚³åˆ° Imgur
                        imgur_url = await upload_to_imgur(screenshot_path)

                        await browser.close()

                        if imgur_url:
                            print(f"ğŸ”— å¿«ç…§é›²ç«¯é€£çµ: {imgur_url}")
                            # å°‡ URL å­˜å…¥çµæœä¸­ï¼Œæ–¹ä¾¿å¾ŒçºŒä½¿ç”¨
                            return {"error": "no_thread_items", "screenshot_url": imgur_url, "url": url,
                                    "html_path": html_path}
                    except Exception as screenshot_error:
                        print(f"âŒ æˆªåœ–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {screenshot_error}")
                        await browser.close()

            # å¦‚æœåŸ·è¡Œåˆ°é€™è£¡è¡¨ç¤ºæ²’æœ‰è¿”å›è³‡æ–™ï¼Œä½†ä¹Ÿæ²’æœ‰éŒ¯èª¤ï¼Œç­‰å¾…å¾Œé‡è©¦
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                print(f"â³ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(wait_time)

        except PlaywrightTimeoutError as e:
            print(f"âŒ è¶…æ™‚éŒ¯èª¤: {e}")

            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 3
                print(f"â³ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(wait_time)

        except Exception as e:
            print(f"âŒ æœªé æœŸçš„éŒ¯èª¤: {e}")

            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                print(f"â³ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(wait_time)

    print(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†")
    return {}


async def fetch_data_from_browser(url: str) -> Tuple[Optional[SnsInfo], Optional[SnsInfo]]:
    """
    å¾ç€è¦½å™¨çˆ¬å–è³‡æ–™ä¸¦è½‰æ›ç‚º SnsInfo

    Returns:
        (ä¸»è²¼æ–‡, å¼•ç”¨è²¼æ–‡) çš„ tupleï¼Œå¤±æ•—æ™‚è¿”å› (None, None)
    """
    main_post = await scrape_thread(url)
    if not main_post:
        print("âŒ ç„¡æ³•çˆ¬å–ä¸»è²¼æ–‡")
        return None, None

    # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤ï¼ˆåªæœ‰ no_thread_items éŒ¯èª¤æœƒæœ‰ screenshot_urlï¼‰
    if main_post.get("error"):
        print(f"âŒ çˆ¬å–å¤±æ•—ï¼ŒéŒ¯èª¤é¡å‹: {main_post['error']}")
        if main_post.get("screenshot_url"):
            print(f"ğŸ“¸ éŒ¯èª¤å¿«ç…§: {main_post['screenshot_url']}")
        return None, None

    quoted_post = None
    if main_post.get("share_info"):
        quoted = main_post["share_info"].get("quoted_post")
        if quoted and quoted.get("code"):
            try:
                quoted_post = convert_to_sns_info(parse_thread(quoted))
            except Exception as e:
                print(f"âš ï¸  è™•ç†å¼•ç”¨è²¼æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return convert_to_sns_info(main_post), quoted_post


def convert_to_sns_info(thread: Dict) -> SnsInfo:
    """å°‡ thread è³‡æ–™è½‰æ›ç‚º SnsInfo ç‰©ä»¶"""
    return SnsInfo(
        post_link=thread["url"],
        profile=Profile(name=thread["username"], url=thread["user_pic"]),
        content=thread["text"],
        images=(thread.get("all_images") or []),
        videos=(thread.get("all_videos") or []),
        attachments=(thread.get("all_attachments") or []),
        timestamp=datetime.fromtimestamp(thread["published_on"])
    )


if __name__ == "__main__":
    # æ¸¬è©¦
    test_url = "https://www.threads.com/@cryforyysh/post/DQBQiuXjv-u"

    print("=" * 60)
    print("ğŸš€ é–‹å§‹çˆ¬å– Threads è²¼æ–‡")
    print("=" * 60)


    async def main():
        sns_info, share_info = await fetch_data_from_browser(test_url)

        if sns_info:
            print("\nâœ… ä¸»è²¼æ–‡:")
            print(sns_info)
        else:
            print("\nâŒ ç„¡æ³•å–å¾—ä¸»è²¼æ–‡")

        if share_info:
            print("\nâœ… å¼•ç”¨è²¼æ–‡:")
            print(share_info)


    asyncio.run(main())